{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Content Creators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Tools\n",
    "3. ML Models\n",
    "    - Text\n",
    "    - Images\n",
    "    - Audio\n",
    "    - Videos\n",
    "4. Final Thoughts\n",
    "5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world of generative AI is a rabbit hole of models capable of spectacular feats of automated \n",
    "content creation. In this workshop, we'll explore some of the most promising genres of generative \n",
    "models with a focus on their creative applications. \n",
    "\n",
    "First we'll look at transformer models - which can conjure up synthetic images, video, and audio \n",
    "that capture the style and essence of their training data.\n",
    "\n",
    "Next up are diffusion models - transformers artsy cousin specialized in realistic image synthesis. \n",
    "We'll talk about how they work their magic and try our hand at guiding them to generate new scenes and \n",
    "characters.\n",
    "\n",
    "Finally, we'll have a look at the different models we can use for Audio generation and Video creation.\n",
    "\n",
    "Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transformers\n",
    "- ctransformers\n",
    "- diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the models below can run in modern-day computers without extravagant hardware. The caveat \n",
    "is that it will most-likely take too lond to run without a decent size GPU. Therefore, we'll walk through this \n",
    "part together, and, where possible, fetch content from different models freely available to generate the input \n",
    "for our website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models like GPT-3 are neural networks trained on massive text datasets to generate \n",
    "language. They are useful for:\n",
    "\n",
    "- Text generation - Creating novels, articles, code, dialogue automatically\n",
    "- Text summarization - Condensing long text into concise summaries \n",
    "- Translation - Converting text between languages\n",
    "- Question answering - Answering natural language questions\n",
    "\n",
    "Large language models absorb linguistic patterns like a sponge soaks up water. After digesting huge \n",
    "volumes of text, they can then wring out new writings dripping with style and substance. Their potential \n",
    "is as vast as the ocean of data they learn from.\n",
    "\n",
    "For this next section, one of the things you'll need to do is to download the checkpoint of the fine-tuned \n",
    "model we will be using.\n",
    "\n",
    "This model was trained by [Mistral AI](https://mistral.ai/) and \n",
    "[fine-tuned on the Open Orca dataset](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF) by TheBloke. \n",
    "It has different variations of it, and the unique part of it is that it uses an implementation in C, which \n",
    "makes it much faster than the Python implementation.\n",
    "\n",
    "‚ö†Ô∏è Please note that the checkpoint is about 4GB in size.\n",
    "\n",
    "```sh\n",
    "huggingface-cli download TheBloke/Mistral-7B-OpenOrca-GGUF mistral-7b-openorca.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "To avoid having to run this on your laptop, you can use this [huggingface space](https://huggingface.co/spaces/osanseviero/mistral-super-fast) to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `gpu_layers` to the number of an adequate number your GPU might be able to handle. Otherwise, \n",
    "setting it to 0 means CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckp  = \"/home/ramonperez/Tresors/seldon/tutorials/synthetic_data/mistral-7b-openorca.Q4_K_M.gguf\"\n",
    "model_name = \"TheBloke/Mistral-7B-OpenOrca-GGUF\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name, model_file=model_ckp, model_type=\"mistral\", gpu_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"Siberian huskies are a fun breed of dogs that\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large vision transformer models like DALL-E are neural networks trained on large amounts of images \n",
    "to generate and manipulate visual content. They are useful for:\n",
    "\n",
    "- Image generation - Creating original digital artwork, photos, and designs \n",
    "- Image editing - Changing attributes like style, lighting, objects in images\n",
    "- Text-to-image - Generating images from textual descriptions\n",
    "- Image captioning - Adding descriptive captions to images\n",
    "\n",
    "Large vision models absorb visual patterns like a painter studies the world around them. After observing \n",
    "enough training data, they can conjure new images imbued with lighting, perspective and detail.\n",
    "\n",
    "Images are a type of unstructured data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "pipeline =  AutoPipelineForText2Image.from_pretrained(\"warp-ai/wuerstchen\", torch_dtype=dtype)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption = \"Anthropomorphic siberian husky dressed as a fire fighter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Black and white cartoon of a siberian husky in a suit sitting at a bar at the top floor of a building in New York City overlooking central park'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline(\n",
    "    prompt=prompt, height=768, width=1024, output_type=\"pil\",\n",
    "    prior_guidance_scale=4.0, decoder_guidance_scale=0.0\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio transformer models like [Jukebox](https://openai.com/research/jukebox) are neural networks trained \n",
    "on large datasets of audio to generate and modify music and speech. They are useful for:\n",
    "\n",
    "- Music generation - Creating original songs, instrumentals, and compositions\n",
    "- Voice cloning - Mimicking the tone and speech patterns of a speaker\n",
    "- Text-to-speech - Converting text to natural sounding speech \n",
    "- Speech enhancement - Improving audio quality, removing background noise\n",
    "\n",
    "Audio transformers absorb acoustic patterns like a musician develops an ear for music. After \"listening\" to \n",
    "enough training samples, they can produce novel rhythms, melodies, and vocals - like an artificial imagination \n",
    "for sound.\n",
    "\n",
    "For our use case, we will be using [MusicGen](https://huggingface.co/facebook/musicgen-small), a model developed by \n",
    "a group of researchers at **Meta AI**.\n",
    "\n",
    "It can help us create novel sounds with only a few words. Let's try it out. üéπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from pedalboard.io import AudioFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"fast bachata with high quality in the style of juan luis guerra\", \"Piano and violin orchestra with slow and fast tempo\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_values[1].numpy(), rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AudioFile(\"../assets/audio/orchestra.wav\", samplerate=sampling_rate, num_channels=1) as f:\n",
    "    f.write(audio_values[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video transformer models are neural networks trained on large video datasets to generate and edit \n",
    "digital video content. They are useful for:\n",
    "\n",
    "- Video generation - Creating original video clips, effects, and animations\n",
    "- Video prediction - Generating plausible future video frames \n",
    "- Text-to-video - Converting text descriptions into video\n",
    "- Video enhancement - Increasing resolution, framerate, quality of video\n",
    "\n",
    "Video transformers absorb motion and temporal patterns like a painter studies light and perspective. \n",
    "After observing enough footage, they can produce novel scenes and effects - like an artificial director's \n",
    "eye for sequencing images. Their potential is as far-reaching as our visual imagination and cinematic history.\n",
    "\n",
    "The model we'll be using is a transformer model created by a group of researchers from Alibaba and its \n",
    "details can be found [here](https://huggingface.co/damo-vilab).\n",
    "\n",
    "A hugging face space can be accessed [here](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "# this setup will work on CPU\n",
    "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float32)\n",
    "\n",
    "# this setup will work on GPU\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\").to('cuda')\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A Siberian Husky is surfing\"\n",
    "prompt = \"A Siberian Husky is going down the street in her skateboard\"\n",
    "video_frames = pipe(prompt, num_inference_steps=25).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = export_to_video(video_frames, output_video_path='../assets/videos/husky_skating.mp4')\n",
    "video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only scratched the surface of what we can do with the current architectures, and we can be confident \n",
    "it is only a matter of time until a new one, or a new combination of old ones, arrives.\n",
    "\n",
    "To recap:\n",
    "- We used pre-trained models for different modalities to generate data for the website we created earlier.\n",
    "- Each model comes with its pros and cons and some have better default settings than others.\n",
    "- By training on thousands of photos, diffusion models learned to render highly realistic synthetic images from noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two exercise templates using the transformers library for a generative models workshop:\n",
    "\n",
    "Exercise 1 - Text Generation\n",
    "\n",
    "Import GPT2 and generate a joke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ____, ____\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('__') \n",
    "tokenizer = _____.____('__')\n",
    "\n",
    "prompt = 'Two elephants walk into a bar' \n",
    "input_ids = tokenizer._____(_____, return_tensors='pt')\n",
    "\n",
    "______ = model.generate(_____, max_length=____)\n",
    "text = tokenizer._____(______[0])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the quality of the joke with that of a joke from pyjokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyjokes\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 - Text Summarization\n",
    "\n",
    "Import T5 and summarize text from wikipedia or elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ____Model, ____Tokenizer\n",
    "\n",
    "model = ____Model.from_pretrained('__')\n",
    "tokenizer = ____Tokenizer.from_pretrained('__') \n",
    "\n",
    "text = \"\"\"Your long text to summarize\"\"\"\n",
    "\n",
    "inputs = _____([text], return_tensors='__')\n",
    "summary = model._____(_____)\n",
    "\n",
    "print(_____._____(_____[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_svcs_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
