{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Engineering Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Learning is synthesizing seemingly divergent ideas and data.â€ ~ Terry Heick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dogs](../images/hr_dogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Tools\n",
    "3. Real Data\n",
    "4. Fake Data\n",
    "5. Synthetic Data\n",
    "6. Use Cases\n",
    "5. Final Thoughts\n",
    "7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee churn refers to the rate at which employees leave a company within a specific time period. To predict churn, \n",
    "companies use data such as employee demographics, job satisfaction surveys, performance metrics, tenure, and historical \n",
    "turnover rates. Analyzing this data helps identify patterns and factors contributing to employee departures, enabling \n",
    "organizations to implement strategies to retain their staff.\n",
    "\n",
    "Now that you know what churn is, imagine that the HR department of the company we've been building comes to you and says, \n",
    "\"a lot of people a churning and I think it would be a good to see if we can predict when people might leave the company \n",
    "in feature. After all, we have over 50,000 employees and we want to make sure we not only make them all happy, but also \n",
    "replace them with the same quality when they take off for their next adventure.\n",
    "\n",
    "That said, I want to request budget for a dedicated People Analytics person but I can't go to the VP of People and ask for more \n",
    "budget without at least something useful information. Remi from the data analytics team offered to help with analyzing the data, \n",
    "but said that it is quite messy at the moment. Could you please help us clean it, and, if possible, automate such cleaning pipeline \n",
    "for future ad-hoc analysis and machine learning use cases?\n",
    "\n",
    "One caveat: We only have a tiny sample of the real data. Since what we need first is a Proof of Concept, would you be \n",
    "able to make it better?\n",
    "\n",
    "What we are going to do is to have a look at the sample provided to us, enhance it with some fake data, and create some \n",
    "cleaning pipelines that we might be able to automate later on. Let's get started! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main tools we'll be using for this section.\n",
    "\n",
    "- [mimesis](https://mimesis.name/en/master/index.html) --> \"Mimesis is a powerful data generator for \n",
    "Python that can produce a wide range of fake data in multiple languages. This tool is useful for populating \n",
    "testing databases, creating fake API endpoints, generating custom structures in JSON and XML files, and \n",
    "anonymizing production data, among other things. With Mimesis, developers can obtain realistic, randomized \n",
    "data easily to facilitate development and testing.\n",
    "- [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) --> \"pandas is a Python package \n",
    "providing fast, flexible, and expressive data structures designed to make working with â€œrelationalâ€ or \n",
    "â€œlabeledâ€ data both easy and intuitive. It aims to be the fundamental high-level building block for doing \n",
    "practical, real-world data analysis in Python.\"\n",
    "- [SDV]() --> Synthetic Data Vault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"sample data\" we were give, we'll use one of the many churn prediction datasets available on Kaggle, and \n",
    "you can find this particular one [here](https://www.kaggle.com/datasets/ninopadilla13/employee-churn?select=employee_churn.csv).\n",
    "\n",
    "It contains the following variables:\n",
    "- `avg_monthly_hrs`: average monthly hours worked per employee\n",
    "- `department`: department where employee works\n",
    "- `filed_complaint`: whether the employee ever filed a complaint\n",
    "- `last_evaluation`: last evaluation score by manager (float between 0.0 and 1.0)\n",
    "- `n_projects`: number of projects the employee has worked on\n",
    "- `recently_promoted`: whether the employee was recently promoted\n",
    "- `salary`: the employee's salary as a categorical variable (low, medium, high)\n",
    "- `satisfaction`: the employee's satisfaction as a categorical variable (float between 0.0 and 1.0)\n",
    "- `status`: the employee's status as a categorical variable (Employed, Left). This would be used as the target \n",
    "class for a machine learning model.\n",
    "- `tenure`: number of years the employee has worked with the company (int between 2 and 10)\n",
    "\n",
    "We will be adding the following variables:\n",
    "- `name`      \n",
    "- `last_name`   \n",
    "- `email`\n",
    "- `address`\n",
    "- `city`\n",
    "- `state`\n",
    "- `country`\n",
    "- `latitude`\n",
    "- `longitude`   \n",
    "- `postal_code`\n",
    "- `number`\n",
    "\n",
    "Let's get started evaluating the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path().cwd().parent.joinpath('data')\n",
    "raw_data = path.joinpath('employee_churn', 'real', 'employee_churn.csv')\n",
    "print(path, '\\n', raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(raw_data)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be quite a few missing values, let's see how many exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isna().sum() / df_raw.shape[0] * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fake Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Monthly Hours (Worked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average monthly hours worked is a ver important proxy (in my opinion) of happiness at work. If I average 310 hours \n",
    "of work per month, I would be doing about 70 hours per week, and that's not sustainable no matter how much one loves \n",
    "its job. There are exceptions to this, of course, but, for the sake of running a successful Tech Business, let's say \n",
    "we'd rather keep our employees at a normal 40 hours per week.\n",
    "\n",
    "Let's see what the actual distribution of hours worked per month is in the small dataset provided to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['avg_monthly_hrs'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that about 25% of the employees in our sample do work about 10/day, and this should probably be flagged as a concern.\n",
    "\n",
    "Let's start creating som fake data based on the characteristics of our sample. For this, we'll use the `Fieldset` \n",
    "and the `Locale` classes from mimesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimesis import Fieldset\n",
    "from mimesis.locales import Locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = Fieldset(locale=Locale.EN, i=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs(\"random.randints\", amount=1, a=155, b=310)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['department'].value_counts(normalize=True, dropna=False) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps_list = df_raw['department'].dropna().unique().tolist()\n",
    "deps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_departments(random, department_names: list) -> Any:\n",
    "    return random.choice(department_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.register_field('department', random_departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimesis.keys import maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('department', department_names=deps_list, key=maybe('nan', probability=0.02))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['department'].value_counts(normalize=True, dropna=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=df_raw['department'].value_counts(normalize=True, dropna=False).to_dict())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['filed_complaint'].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices={1: 0.15, 'nan': 0.85})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['last_evaluation'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.uniform', a=0.35, b=1.0, precision=5)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_projects = df_raw['n_projects'].value_counts(normalize=True).to_dict()\n",
    "n_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=n_projects)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recently Promoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promos = df_raw['recently_promoted'].value_counts(normalize=True, dropna=False).to_dict()\n",
    "promos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=promos)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_cat = df_raw['salary'].value_counts(normalize=True, dropna=False).to_dict()\n",
    "salaries_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=salaries_cat)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['satisfaction'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.uniform', a=0.45, b=1.0, precision=4, key=maybe(0.10, probability=0.05))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable: Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_raw['status'].value_counts(normalize=True, dropna=False).to_dict()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=target)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure = df_raw['tenure'].value_counts(normalize=True, dropna=True).to_dict()\n",
    "tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs('random.weighted_choice', choices=tenure)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name        = fs('name')\n",
    "last_name   = fs('last_name')\n",
    "email       = fs('email', domains=['creativeagency.com'])\n",
    "address     = fs('address')\n",
    "city        = fs('city')\n",
    "state       = fs('state')\n",
    "country     = fs('default_country')\n",
    "latitude    = fs('latitude')\n",
    "longitude   = fs('longitude')\n",
    "postal_code = fs('postal_code')\n",
    "number      = fs('phone_number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = Fieldset(locale=Locale.EN, i=2000)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"___\": ____,\n",
    "    \"___\": ____,\n",
    "    ...\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.read_csv(raw_data)\n",
    "df_real.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import get_available_demos\n",
    "\n",
    "get_available_demos(modality='single_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.local import load_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_csvs(folder_name=raw_data.parent)\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_table = datasets['employee_churn']\n",
    "type(house_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata = SingleTableMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.detect_from_dataframe(data=house_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_dict = metadata.to_dict()\n",
    "python_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.visualize(\n",
    "    show_table_details='summarized',\n",
    "    output_filepath='my_metadata.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='has_rewards',\n",
    "    sdtype='boolean') # categorical and numerical go the same way but can have computer_representation='Float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='checkin_date',\n",
    "    sdtype='datetime',\n",
    "    datetime_format='%d %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='billing_address',\n",
    "    sdtype='address',\n",
    "    pii=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.set_primary_key(column_name='guest_email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.save_to_json(filepath='my_metadata_v1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata_obj = SingleTableMetadata.load_from_dict(metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = GaussianCopulaSynthesizer(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.fit(df_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = synthesizer.sample(num_rows=2000)\n",
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.save(\n",
    "    filepath='../models/my_synthesizer.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "quality_report = evaluate_quality(\n",
    "    real_data=df_real,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.shape, df_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report.get_details(property_name='Column Shapes')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "diagnostic_report = run_diagnostic(\n",
    "    real_data=df_real,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_report.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_report.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_report.get_details(property_name='Coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import get_column_plot\n",
    "\n",
    "fig = get_column_plot(\n",
    "    real_data=df_real,\n",
    "    synthetic_data=synthetic_data,\n",
    "    column_name='____',\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import get_column_pair_plot\n",
    "\n",
    "fig = get_column_pair_plot(\n",
    "    real_data=df_real,\n",
    "    synthetic_data=synthetic_data,\n",
    "    column_names=['____', '____'],\n",
    "    metadata=metadata)\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both fake data and synthetic data are useful techniques for bootstrapping analyses, creating data engineering pipelines \n",
    "and building machine learning models.\n",
    "\n",
    "Synthetic Data helps us anonymize data and still get the benefits of analyzing personally identifiable information.\n",
    "\n",
    "Fake data can serve as an excellent placeholder when building applications that require that we have a populated database \n",
    "in place for testing purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_svcs_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
